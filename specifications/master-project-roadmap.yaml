openapi: 3.1.0
info:
  title: Gapless Network Data - Master Project Roadmap
  version: 0.1.0
  description: |
    Single Source of Truth (SSoT) for gapless-network-data project planning.
    Coordinates all phases, specifications, and implementation work.

    This is the master coordination file that links all sub-specifications:
    - documentation-audit-phase.yaml (completed 2025-11-03)
    - duckdb-integration-strategy.yaml (planning complete 2025-11-03)
    - Future phase specifications

    Architecture: OpenAPI 3.1.1 machine-readable format
    Pattern: Logical dependencies (capabilities, not time-based)
    Evolution: Dynamic - findings tracked in x-implementation-findings

  x-metadata:
    project_name: "gapless-network-data"
    project_version: "v0.1.0 (alpha)"
    current_phase: "Phase 0: Foundation (complete), Phase 1: Basic Collection (planned)"
    next_milestone: "v0.2.0 (Ethereum PRIMARY + Bitcoin SECONDARY collection)"
    last_updated: "2025-11-08T03:00:00Z"
    supersedes: ["duckdb-integration-strategy.yaml (implementation-focused roadmap)"]
    focus_shift: |
      1. Feature-driven planning (WHAT to collect) vs architecture-driven (HOW to implement)
      2. Ethereum BigQuery (instant 5-year historical) as PRIMARY, Bitcoin mempool.space (5min low-frequency) as SECONDARY
      3. Chronological phase ordering: Collection → Quality → Features → Expansion → Production → Release

  x-architecture:
    core_principle: "DuckDB PRIMARY for Raw Data Storage"
    rationale: |
      Store all raw blockchain data in DuckDB tables (single data.duckdb file).
      Provides maximum flexibility for historical data + feature engineering.

    benefits:
      - "Maximum flexibility: Direct SQL resampling, temporal joins, window functions"
      - "10-100x faster: DuckDB SQL vs Pandas for feature engineering"
      - "Simplicity: Single data.duckdb file (~1.5 GB for 5 years Ethereum)"
      - "Compact storage: ~1.5 GB DuckDB vs ~1.2 GB Parquet (negligible 300 MB difference)"
      - "Production-proven (DoorDash uses DuckDB for 1-min time-series)"

  x-slos:
    availability:
      target: "Data collection operations complete without manual intervention"
      measurement: "Percentage of collection runs that complete successfully with checkpoint/resume"
      validation: "Checkpoint-based resume survives crashes, batch operations handle errors gracefully"
      failure_mode: "Collection hangs, requires manual restart, loses progress on crash"
      error_handling: "Raise and propagate exceptions, no fallbacks/defaults/silent handling"

    correctness:
      target: "100% data accuracy"
      measurement: "All validation layers pass, no silent errors"
      validation: "5-layer validation pipeline + DuckDB CHECK constraints"
      failure_mode: "Exception-only failures (no defaults, no fallbacks)"
      error_handling: "Structured exceptions with context, no silent data corruption"

    observability:
      target: "100% operation tracking"
      measurement: "All data collection and validation logged"
      log_format: "timestamp, operation, status, duration, errors"
      storage: "Validation reports with queryable history"
      failure_mode: "Silent failures, missing audit trail"

    maintainability:
      target: "<30 minutes for common operations"
      measurement: "Time to add new validation layer, new collector, new query"
      documentation: "Hub-and-spoke with progressive disclosure (CLAUDE.md)"
      failure_mode: "Undocumented patterns, unclear architecture"

    exclusions:
      note: "Performance/speed, security, and availability SLAs explicitly excluded"
      rationale: "Focus on correctness and observability; performance optimization deferred to production phases"

x-project-phases:
  phase_0_foundation:
    name: "Foundation & Research"
    status: "completed"
    duration: "2025-10-28 to 2025-11-03"
    completion_date: "2025-11-03"

    objectives:
      - "Establish package structure and SDK quality standards"
      - "Research data sources (mempool.space, LlamaRPC)"
      - "Document collector patterns from GitHub projects"
      - "Audit documentation quality (5-agent investigation)"
      - "Investigate DuckDB opportunities (5-agent investigation)"

    deliverables:
      - status: "completed"
        item: "Package structure with PEP 561 compliance (py.typed)"
      - status: "completed"
        item: "API interface (fetch_snapshots, get_latest_snapshot)"
      - status: "completed"
        item: "Structured exceptions (HTTP, Validation, RateLimit)"
      - status: "completed"
        item: "Retry logic (exponential backoff, max 3 retries)"
      - status: "completed"
        item: "LlamaRPC research (52 files, ~1MB documentation)"
      - status: "completed"
        item: "Collector patterns documentation (28 patterns)"
      - status: "completed"
        item: "Documentation audit (6 findings, all resolved)"
      - status: "completed"
        item: "DuckDB investigation (23 features, 110x savings, 10-100x speedups)"

    findings:
      architecture:
        - "Referential implementation: gapless-crypto-data provides proven ValidationStorage pattern"
        - "DuckDB optimal for time-series analytics (not just storage)"
        - "Parquet better for immutable data (110x smaller than DuckDB tables)"
        - "ASOF JOIN prevents data leakage (critical for trading models)"

      data_sources:
        - "mempool.space: M5 recent, H12 historical (Bitcoin)"
        - "BigQuery: Instant 5-year historical (2015+ archive, Ethereum public dataset)"
        - "RPC streaming deferred to Phase 2+ (real-time forward collection)"
        - "No multi-source redundancy for Bitcoin (single public API)"

      documentation:
        - "Blob calculations outdated (EIP-7691: 6→9 blobs)"
        - "Collector patterns 54% complete (missing tradeoffs, 4 patterns)"
        - "Applicability claims 77% accurate (Ethereum→Bitcoin requires adaptation)"

    success_gates:
      - gate: "SDK quality standards defined"
        status: "passed"
        evidence: "PEP 561 compliance, structured exceptions, type stubs"

      - gate: "Data sources validated"
        status: "passed"
        evidence: "mempool.space REST API confirmed (10 req/sec, no auth)"

      - gate: "Architecture principle established"
        status: "passed"
        evidence: "Parquet for Data, DuckDB for Queries (investigation-backed)"

      - gate: "Documentation quality acceptable"
        status: "passed"
        evidence: "All 6 audit findings resolved, 85% compliance"

  phase_1_basic_collection:
    name: "Historical Data Collection (5-Year Backfill)"
    status: "planned"
    estimated_duration: "2-4 hours total (< 1 hour data acquisition + 2-3 hours implementation)"
    dependencies: ["phase_0_foundation"]
    validation_status: "Empirically validated 2025-11-07 - see .claude/skills/bigquery-ethereum-data-acquisition/"

    objectives:
      - "Collect 5 years of Ethereum block data (2020-2025, ~13M blocks) via BigQuery - PRIMARY HISTORICAL BACKFILL"
      - "Store in DuckDB PRIMARY (single data.duckdb file, ~1.5 GB for Ethereum)"
      - "Instant bulk download (< 1 hour total, 624x faster than RPC polling)"
      - "Basic validation (schema, constraints)"
      - "Bitcoin collection deferred to Phase 2+ (SECONDARY priority)"

    priority_note: "BigQuery provides instant historical backfill (<1 hour). Real-time forward collection (streaming updates) deferred to Phase 2+."

    collection_mode: "historical_bulk"
    architectural_decision: |
      Phase 1 uses Google BigQuery public dataset for instant 5-year historical backfill.
      This allows building complete historical dataset in <1 hour, then adding real-time
      streaming updates in Phase 2.

      Rationale:
      - BigQuery: 624x faster than RPC polling (< 1 hour vs 26 days)
      - Free tier: 1 TB/month query limit (0.97 GB for 13M blocks = 0.1% usage)
      - Complete dataset: All 13M blocks instantly available from public dataset
      - Column optimization: 11 ML-optimized columns = 97% cost savings vs all 23 columns
      - Real-time streaming can be added as incremental feature in Phase 2

    features:
      ethereum_historical_backfill:
        name: "Ethereum 5-Year Historical Backfill via BigQuery"
        priority: "P0"
        effort: "2-4 hours total (< 1 hour data download + 2-3 hours implementation)"
        status: "pending"
        detailed_specification: "/Users/terryli/eon/gapless-network-data/.claude/skills/bigquery-ethereum-data-acquisition/"

        what_to_collect:
          - "5 years of Ethereum blocks (2020-01-01 to 2025-01-01, ~13M blocks)"
          - "Google BigQuery public dataset: bigquery-public-data.crypto_ethereum.blocks"
          - "11 ML-optimized columns: timestamp, number, gas_limit, gas_used, base_fee_per_gas, transaction_count, difficulty, total_difficulty, size, blob_gas_used, excess_blob_gas"
          - "Store in: DuckDB ethereum_blocks table (~1.5 GB, 62 bytes/row empirically validated)"

        tasks:
          - task: "One-time setup"
            effort: "30 minutes"
            details:
              - "Install gcloud CLI and authenticate: gcloud auth application-default login"
              - "Install Python dependencies: google-cloud-bigquery, pandas, pyarrow, db-dtypes"
              - "Verify authentication with test query"

          - task: "Cost validation"
            effort: "5 minutes"
            details:
              - "Run test_bigquery_cost.py to validate query cost (0.97 GB)"
              - "Confirm 0.1% of 1 TB free tier usage"
              - "Document actual vs expected cost"

          - task: "Bulk download to Parquet"
            effort: "30-60 minutes (network-bound)"
            details:
              - "Run download_bigquery_to_parquet.py with block range (11560000-24000000)"
              - "Stream results directly to Parquet file (no BigQuery storage used)"
              - "Validate output: 13M rows, ~760 MB file size"

          - task: "DuckDB import"
            effort: "1-2 hours"
            details:
              - "Create ethereum_blocks table with schema and constraints"
              - "Load Parquet file into DuckDB: CREATE TABLE AS SELECT FROM read_parquet()"
              - "Execute CHECKPOINT for durability"
              - "Verify data integrity (row count, column types, constraint checks)"
              - "Test basic queries (time_bucket aggregations, window functions)"

          - task: "Multi-chain API"
            effort: "1-2 hours"
            details:
              - "Add chain parameter: fetch_snapshots(chain='bitcoin'|'ethereum')"
              - "Single DuckDB database, separate tables (ethereum_blocks, bitcoin_mempool)"
              - "Chain-specific data access (dispatch by chain)"

        acceptance_criteria:
          - "Successfully download 5 years of Ethereum blocks (~13M blocks)"
          - "Data stored in DuckDB ethereum_blocks table"
          - "100% data completeness (no missing blocks in BigQuery public dataset)"
          - "Query cost ≤ 1 GB (< 0.1% of free tier)"
          - "Total time < 2 hours (setup + download + import)"
          - "Schema validation catches missing/malformed fields"
          - "Multi-chain API works: fetch_snapshots(chain='ethereum', mode='historical')"

      bitcoin_historical_collection:
        name: "Bitcoin Mempool Historical Data"
        priority: "P1"
        effort: "8-12 hours"
        status: "pending"

        what_to_collect:
          - "5 years of Bitcoin mempool data (H12 granularity, 3,650 snapshots)"
          - "mempool.space historical API (12-hour intervals for data > 1 month old)"
          - "9 fields: timestamp, unconfirmed_count, vsize_mb, total_fee_btc, 5 fee rates"
          - "Store in: DuckDB bitcoin_mempool table (~5 MB)"

        limitation: |
          Bitcoin mempool.space provides:
          - M5 (5-minute) granularity for recent data (< 1 month)
          - H12 (12-hour) granularity for historical data (> 1 month)
          This means 99.3% data loss for historical collection (5-min → 12-hour).

        decision: |
          Collect H12 historical data despite low granularity.
          Rationale: Some historical context better than none for feature engineering.
          Can add recent M5 data later for improved granularity.

        tasks:
          - task: "Implement historical mempool collector"
            effort: "3-4 hours"
            details:
              - "Fetch historical snapshots from mempool.space API"
              - "Parse H12 (12-hour) granularity data"
              - "Extract 9 fields from JSON response"
              - "DuckDB batch INSERT (100 snapshots per transaction)"

          - task: "Basic validation (Layer 1-2 only)"
            effort: "2-3 hours"
            details:
              - "Layer 1: HTTP validation (200 OK, timeout handling)"
              - "Layer 2: Schema validation (all 9 fields present, correct types)"
              - "Raise structured exceptions on failure"

          - task: "Testing"
            effort: "3-5 hours"
            details:
              - "Unit tests with mocked API responses"
              - "Integration test: collect 100 real historical snapshots"
              - "Coverage: 70%+ on bitcoin collector"

        acceptance_criteria:
          - "Successfully collect 5 years of Bitcoin mempool data (3,650 snapshots)"
          - "Data stored in DuckDB bitcoin_mempool table"
          - "HTTP errors handled with retry logic"
          - "Schema validation catches missing/malformed fields"
          - "Granularity limitation documented (H12 for historical)"

    deliverables:
      - "5 years of Ethereum block data (2020-2025, ~13M blocks, ~1.5 GB in DuckDB)"
      - "DuckDB PRIMARY storage (single data.duckdb file, empirically validated)"
      - "BigQuery bulk download workflow (< 1 hour total, 624x faster than RPC)"
      - "11 ML-optimized columns (97% cost savings vs all 23 columns)"
      - "Parquet intermediate format (zero-copy DuckDB import)"
      - "Basic validation (schema + constraints)"
      - "Bitcoin collection deferred to Phase 2+"

    success_gates:
      - gate: "Ethereum historical backfill complete (PRIMARY)"
        criteria:
          - "5 years of Ethereum blocks collected (~13M blocks)"
          - "100% data completeness (BigQuery public dataset)"
          - "Data stored in DuckDB ethereum_blocks table"
          - "Query cost < 1 GB (< 0.1% of free tier)"
          - "Total time < 2 hours (setup + download + import)"
          - "Basic validation (schema + constraints) operational"

      - gate: "Bitcoin historical collection complete (SECONDARY)"
        criteria:
          - "5 years of Bitcoin mempool data collected (3,650 snapshots)"
          - "Data stored in DuckDB bitcoin_mempool table"
          - "mempool.space integration working"
          - "H12 granularity limitation documented"
          - "Basic validation (HTTP + schema) operational"

      - gate: "Multi-chain API working"
        criteria:
          - "fetch_snapshots(chain='ethereum', mode='historical') works"
          - "fetch_snapshots(chain='bitcoin', mode='historical') works"
          - "Single DuckDB storage: data.duckdb with ethereum_blocks and bitcoin_mempool tables"
          - "Chain-specific collectors dispatched correctly"

  phase_2_data_quality:
    name: "Data Quality & Validation (Prove It's Reliable)"
    status: "planned"
    estimated_duration: "1-2 weeks"
    dependencies: ["phase_1_basic_collection"]

    objectives:
      - "Complete 5-layer validation pipeline"
      - "Gap detection and automatic backfill"
      - "Zero-gap guarantee for Ethereum (high-frequency source)"
      - "ValidationStorage for audit trail"

    features:
      validation_pipeline:
        name: "5-Layer Validation Pipeline"
        priority: "P0"
        effort: "6-8 hours"

        layers:
          - layer: "Layer 1 - HTTP/RPC Validation (already implemented)"
          - layer: "Layer 2 - Schema Validation (already implemented)"
          - layer: "Layer 3 - Sanity Checks"
            details:
              - "Ethereum: baseFeePerGas > 0, gasUsed <= gasLimit, timestamp increasing"
              - "Bitcoin: fee ordering (fastest >= half_hour >= hour >= economy >= minimum)"
              - "Value range checks"
          - layer: "Layer 4 - Gap Detection"
            details:
              - "Detect missing intervals (Ethereum: >15s gaps, Bitcoin: >6min gaps)"
              - "Use DuckDB LAG() window function (20x faster than Python)"
              - "Return gap ranges with start, end, duration, missing_count"
          - layer: "Layer 5 - Anomaly Detection"
            details:
              - "Z-score anomaly detection on key metrics"
              - "Ethereum: baseFeePerGas spikes, gasUsed anomalies"
              - "Bitcoin: vsize spikes, fee rate anomalies"
              - "Use DuckDB QUALIFY clause (10x faster than pandas)"

        tasks:
          - task: "Implement Layer 3 sanity checks"
            effort: "2 hours"
          - task: "Implement Layer 4 gap detection (DuckDB LAG)"
            effort: "2-3 hours"
          - task: "Implement Layer 5 anomaly detection (DuckDB QUALIFY)"
            effort: "2-3 hours"

      gap_backfill:
        name: "Automatic Gap Backfill"
        priority: "P0"
        effort: "6-8 hours"

        what_to_backfill:
          - "Ethereum: Use LlamaRPC to fetch missing blocks by number"
          - "Bitcoin: Limited by mempool.space granularity (5min recent, 12hr historical)"

        tasks:
          - task: "Ethereum historical backfill"
            effort: "4-5 hours"
            details:
              - "Detect gaps in block sequence"
              - "Fetch missing blocks by number: eth_getBlockByNumber(block_num)"
              - "Parallel requests with rate limiting"
              - "Progress tracking and resume capability"
              - "Validation for backfilled data"

          - task: "Bitcoin backfill (limited)"
            effort: "2-3 hours"
            details:
              - "Detect gaps in 5-minute snapshots"
              - "Attempt backfill from mempool.space historical API (12hr granularity)"
              - "Document granularity limitations"

      validation_storage:
        name: "ValidationStorage for Audit Trail"
        priority: "P0"
        effort: "4-5 hours"

        what_to_store:
          - "Validation reports: timestamp, layer, severity, status, details"
          - "Store as Parquet files: validation_reports_YYYYMMDD.parquet"
          - "Queryable with DuckDB (110x smaller than DuckDB tables)"

        tasks:
          - task: "Implement Parquet-backed ValidationStorage"
            effort: "3-4 hours"
            details:
              - "Write validation reports to Parquet (daily files)"
              - "Query interface using DuckDB read_parquet()"
              - "Aggregation queries (error trending, layer performance)"

          - task: "Validation report viewer"
            effort: "1-2 hours"
            details:
              - "CLI command: validate view --date=YYYY-MM-DD"
              - "Show validation summary (errors by layer, severity)"
              - "Filter by chain, layer, severity"

    deliverables:
      - "5-layer validation pipeline operational"
      - "Gap detection for Ethereum (12s granularity)"
      - "Automatic backfill for Ethereum (historical blocks)"
      - "ValidationStorage with Parquet backend"
      - "Zero-gap guarantee for Ethereum"

    success_gates:
      - gate: "Validation pipeline complete"
        criteria:
          - "All 5 layers operational"
          - "Sanity checks enforce data quality rules"
          - "Gap detection finds 100% of missing intervals"
          - "Anomaly detection flags outliers (z-score > 3)"

      - gate: "Zero-gap guarantee achieved"
        criteria:
          - "Ethereum: No gaps >15 seconds in collected data"
          - "Automatic backfill recovers missing blocks"
          - "Bitcoin: Acknowledged 5-min granularity limitation"

      - gate: "ValidationStorage operational"
        criteria:
          - "Validation reports stored in Parquet"
          - "Query interface working (DuckDB)"
          - "CLI validation viewer functional"
          - "Documentation complete"

  phase_3_feature_engineering:
    name: "Feature Engineering (Prove It's Useful)"
    status: "planned"
    estimated_duration: "1-2 weeks"
    dependencies: ["phase_2_data_quality"]

    objectives:
      - "Temporal alignment with OHLCV data (prevent data leakage)"
      - "Cross-domain features (Ethereum gas + price, Bitcoin mempool + price)"
      - "Production-ready examples for ML pipelines"
      - "Integration with gapless-crypto-data"

    features:
      temporal_alignment:
        name: "Temporal Alignment with OHLCV"
        priority: "P0"
        effort: "6-8 hours"

        why_needed: "Align network data (Ethereum gas, Bitcoin mempool) with price data (OHLCV) for feature engineering. Must prevent data leakage (no lookahead bias)."

        tasks:
          - task: "Implement align_with_ohlcv() function"
            effort: "3-4 hours"
            details:
              - "Use DuckDB ASOF JOIN for forward-fill alignment"
              - "Prevents data leakage: ON ohlcv.timestamp >= network.timestamp"
              - "16x faster than pandas reindex (800ms → 50ms)"
              - "Works for both Ethereum and Bitcoin data"

          - task: "Integration with gapless-crypto-data"
            effort: "2-3 hours"
            details:
              - "Test alignment with BTCUSDT OHLCV (1-minute)"
              - "Test alignment with ETHUSDT OHLCV (1-minute)"
              - "Verify no lookahead bias (unit tests)"
              - "Performance benchmarks vs pandas"

          - task: "Documentation and examples"
            effort: "1-2 hours"
            details:
              - "docs/guides/FEATURE_ENGINEERING.md"
              - "Example: Ethereum gas + ETHUSDT OHLCV"
              - "Example: Bitcoin mempool + BTCUSDT OHLCV"

      cross_domain_features:
        name: "Cross-Domain Feature Examples"
        priority: "P0"
        effort: "6-8 hours"

        features_to_implement:
          ethereum:
            - "Gas pressure ratio: baseFeePerGas / historical_median"
            - "Block utilization: gasUsed / gasLimit"
            - "Gas-adjusted returns: (price_change * gasLimit) / gasUsed"
            - "Congestion z-score: (gasUsed - mean) / std"
            - "Transaction velocity: transactions.length / block_time"

          bitcoin:
            - "Fee pressure ratio: fastest_fee / economy_fee"
            - "Mempool congestion z-score: (unconfirmed_count - mean) / std"
            - "Volume per transaction: OHLCV_volume / unconfirmed_count"
            - "Fee efficiency: total_fee_btc / vsize_mb"

          cross_chain:
            - "Relative network activity: ETH_gas_used / BTC_mempool_size"
            - "Fee correlation: ETH_baseFee vs BTC_fastest_fee"

        tasks:
          - task: "Implement 10+ cross-domain features"
            effort: "4-5 hours"
          - task: "Create 43-feature engineering script"
            effort: "2-3 hours"
            details:
              - "Update existing script with real Ethereum + Bitcoin data"
              - "Demonstrate full feature engineering pipeline"
              - "Output ready-to-use feature matrix for ML"

    deliverables:
      - "Temporal alignment API (align_with_ohlcv function)"
      - "10+ documented cross-domain features"
      - "Working examples: Ethereum + ETHUSDT, Bitcoin + BTCUSDT"
      - "43-feature engineering script"
      - "docs/guides/FEATURE_ENGINEERING.md"

    success_gates:
      - gate: "Temporal alignment working"
        criteria:
          - "align_with_ohlcv() prevents data leakage (verified)"
          - "10x+ faster than pandas reindex"
          - "Works for both Ethereum and Bitcoin"
          - "Unit tests verify no lookahead bias"

      - gate: "Cross-domain features ready"
        criteria:
          - "At least 10 features implemented and documented"
          - "Working examples for both chains"
          - "43-feature script generates ML-ready output"
          - "Documentation complete with usage examples"

  phase_4_multi_chain_expansion:
    name: "Multi-Chain Expansion (Scale Horizontally)"
    status: "future"
    estimated_duration: "2-3 weeks"
    dependencies: ["phase_3_feature_engineering"]

    objectives:
      - "Add more blockchain data sources (Solana, Avalanche, Polygon, etc.)"
      - "Expand Ethereum metrics (full 26-field block schema)"
      - "Historical backfill for all chains (where available)"

    what_to_collect:
      solana:
        - "Block-level data (slot, blockHeight, blockTime, transactions)"
        - "Network metrics (TPS, block times, fee rates)"
        - "Source: Helius RPC or public Solana RPC"
        - "Granularity: ~400ms block time (ultra high-frequency)"

      avalanche:
        - "C-Chain block data (number, timestamp, gasUsed, gasLimit, baseFeePerGas)"
        - "Source: Avalanche public RPC"
        - "Granularity: ~2s block time"

      polygon:
        - "PoS chain block data (same schema as Ethereum)"
        - "Source: Polygon public RPC"
        - "Granularity: ~2s block time"

      ethereum_expanded:
        - "Full 26-field block schema (from LlamaRPC research)"
        - "20+ derived metrics (difficulty, totalDifficulty, size, uncles, etc.)"
        - "Transaction-level metrics (average gas price, tx count by type)"

    deliverables:
      - "Support for 5+ blockchains (Ethereum, Bitcoin, Solana, Avalanche, Polygon)"
      - "Full 26-field Ethereum schema implemented"
      - "Historical backfill for Ethereum (2015-2025, ~12M blocks)"
      - "Unified multi-chain API"

  phase_5_production_features:
    name: "Production Features (CLI, Monitoring, Alerts)"
    status: "future"
    estimated_duration: "2-3 weeks"
    dependencies: ["phase_4_multi_chain_expansion"]

    objectives:
      - "Production-ready CLI (stream, backfill, validate, export)"
      - "Real-time monitoring and alerting"
      - "Performance optimization for large datasets"

    features:
      production_cli:
        - "stream command: Real-time continuous collection"
        - "backfill command: Historical data with progress tracking"
        - "validate command: View validation reports"
        - "export command: Multi-format export (CSV, JSON, Arrow)"

      monitoring_alerts:
        - "Real-time anomaly detection (fee spikes, congestion events)"
        - "Custom alerting rules (user-defined thresholds)"
        - "Alert delivery (email, webhook, Telegram)"
        - "Network health dashboard queries"

      performance:
        - "DuckDB query optimization for multi-year datasets"
        - "Remote Parquet access (S3, CDN) via httpfs extension"
        - "Parallel query execution"
        - "10x+ speedup for large aggregations"

    deliverables:
      - "Full-featured CLI with 4 commands"
      - "Real-time alerting system"
      - "Performance optimization (10x+ for large datasets)"
      - "Remote data access capability"

  phase_6_release:
    name: "Release v1.0.0"
    status: "future"
    estimated_duration: "2-3 weeks"
    dependencies: ["phase_5_production_features"]

    objectives:
      - "Complete documentation (all 9 pending docs)"
      - "70%+ test coverage"
      - "CI/CD automation"
      - "PyPI publishing"
      - "Community resources"

    scope:
      documentation:
        - "docs/architecture/OVERVIEW.md - System architecture and data flow"
        - "docs/architecture/DATA_FORMAT.md - Multi-chain data schemas"
        - "docs/guides/DATA_COLLECTION.md - Collection guides for all supported chains"
        - "docs/guides/python-api.md - Complete API reference with examples"
        - "docs/guides/FEATURE_ENGINEERING.md - Cross-domain feature engineering"
        - "docs/validation/OVERVIEW.md - 5-layer validation system"
        - "docs/validation/STORAGE.md - Validation storage and query patterns"
        - "docs/development/SETUP.md - Development environment setup"
        - "docs/development/PUBLISHING.md - Release and publishing workflow"

      testing:
        - "70%+ test coverage (SDK, collectors, validation)"
        - "Unit tests for all public APIs"
        - "Integration tests (end-to-end collection + validation)"
        - "Multi-chain tests (Bitcoin, Ethereum, alt-L1s)"
        - "Performance regression tests"

      ci_cd:
        - "GitHub Actions workflow (test, lint, type-check)"
        - "PyPI trusted publishing setup"
        - "Pre-commit hooks (ruff, mypy, pytest)"
        - "Automated releases with semantic versioning"
        - "Documentation auto-generation"

      community:
        - "README.md with quick-start examples"
        - "CONTRIBUTING.md guidelines"
        - "Example notebooks (Jupyter) for common use cases"
        - "Video tutorials (data collection, feature engineering)"
        - "Discord/Slack community setup"

    deliverables:
      - "Complete documentation (100% of pending docs)"
      - "70%+ test coverage across all modules"
      - "CI/CD pipeline operational"
      - "PyPI package published (v1.0.0)"
      - "Community onboarding resources"

    success_gates:
      - gate: "Documentation complete"
        criteria:
          - "All 9 pending documentation files written"
          - "API reference complete with examples"
          - "Multi-chain guides for all supported chains"
          - "Feature engineering examples working"

      - gate: "Testing complete"
        criteria:
          - "70%+ coverage achieved"
          - "All tests passing (unit + integration)"
          - "Multi-chain tests for all supported chains"
          - "Performance regression tests in place"

      - gate: "CI/CD operational"
        criteria:
          - "GitHub Actions workflow running"
          - "PyPI trusted publishing configured"
          - "Pre-commit hooks working"
          - "Automated releases functional"

      - gate: "v1.0.0 release ready"
        criteria:
          - "All success gates passed"
          - "PyPI package published"
          - "Community resources available"
          - "No critical bugs in issue tracker"

x-sub-specifications:
  documentation_audit:
    file: "/Users/terryli/eon/gapless-network-data/specifications/documentation-audit-phase.yaml"
    status: "completed"
    completion_date: "2025-11-03"
    scope: "Documentation quality audit and fixes"
    findings:
      - "Blob calculations outdated (EIP-7691)"
      - "Collector patterns 54% complete"
      - "Applicability claims 77% accurate"
      - "YAML frontmatter missing"
      - "File paths missing trailing spaces"
    resolution: "All 6 findings resolved, compliance 62% → 85%"

  duckdb_integration:
    file: "/Users/terryli/eon/gapless-network-data/specifications/duckdb-integration-strategy.yaml"
    status: "planning complete"
    scope: "DuckDB integration strategy and implementation"
    findings:
      - "23 DuckDB features discovered"
      - "110x storage savings with Parquet"
      - "10-100x performance gains"
      - "Production evidence: DoorDash case study"
      - "ASOF JOIN prevents data leakage"
    priorities:
      phase_1: "6 high-priority features (14-19 hours)"
      phase_2: "Analytics & optimization (10-13 hours)"
      phase_3: "Advanced features (5-8 hours)"

  core_collection_phase1:
    file: "/Users/terryli/eon/gapless-network-data/specifications/archive/core-collection-phase1.yaml"
    status: "superseded"
    superseded_date: "2025-11-04"
    superseded_by: "master-project-roadmap.yaml Phase 1 (multi-chain)"
    scope: "Bitcoin-only Phase 1 implementation (3/6 tasks completed)"
    reason: "Conflicts with Option A decision (Ethereum PRIMARY + Bitcoin multi-chain)"
    reusable_content:
      - "Retry logic patterns (tenacity library, exponential backoff)"
      - "Structured exceptions (HTTP, Validation, RateLimit)"
      - "Forward-collection enforcement (validates start time within 5 minutes)"
    note: "Archived for reference; Bitcoin collector patterns reused in multi-chain Phase 1"

  ethereum_collector_phase1:
    file: "/Users/terryli/eon/gapless-network-data/.claude/skills/bigquery-ethereum-data-acquisition/"
    status: "active"
    created_date: "2025-11-08"
    updated_date: "2025-11-08"
    supersedes: "RPC-based ethereum-collector-phase1.yaml (archived)"
    scope: "Ethereum historical data acquisition via Google BigQuery public dataset (PRIMARY data source)"
    priority: "P0"
    deliverables:
      - "BigQuery bulk download workflow (< 1 hour total)"
      - "11 ML-optimized columns (97% cost savings)"
      - "Parquet intermediate format with DuckDB import"
      - "Multi-chain API integration (chain parameter)"
      - "Basic validation (schema + constraints)"
      - "Comprehensive tests (70%+ coverage)"
    estimated_effort: "2-4 hours total (< 1 hour download + 2-3 hours implementation)"
    reference: "Part of Phase 1 multi-chain implementation (v0.2.0)"

  duckdb_schema:
    file: "/Users/terryli/eon/gapless-network-data/specifications/duckdb-schema-specification.yaml"
    status: "active"
    created_date: "2025-11-04"
    scope: "DuckDB PRIMARY storage schema for ethereum_blocks and bitcoin_mempool tables"
    priority: "P0"
    content:
      - "DDL statements with constraints and indexes"
      - "Common query patterns (time_bucket, ASOF JOIN, window functions)"
      - "Data integrity checks (gap detection, duplicates, invalid values)"
      - "Performance tuning guidelines"
    storage_architecture:
      location: "~/.cache/gapless-network-data/data.duckdb"
      size: "~1.5 GB for 5 years (13M Ethereum blocks at 76-100 bytes/block, empirically validated)"
      tables:
        - "ethereum_blocks (BIGINT PRIMARY KEY, 6 fields)"
        - "bitcoin_mempool (INTEGER PRIMARY KEY, 9 fields) - deferred to Phase 2+"
        - "metadata (checkpoints and collection progress)"

  historical_collection_strategy:
    file: "/Users/terryli/eon/gapless-network-data/.claude/skills/bigquery-ethereum-data-acquisition/"
    status: "active"
    created_date: "2025-11-08"
    updated_date: "2025-11-08"
    supersedes: "RPC-based HISTORICAL_COLLECTION_STRATEGY.md (archived 2025-11-08)"
    scope: "5-year historical data backfill strategy via BigQuery (2020-2025)"
    priority: "P0"
    architectural_decision: "BigQuery bulk download for historical backfill (< 1 hour, 624x faster than RPC). Real-time streaming deferred to Phase 2+."
    content:
      - "Ethereum 5-year backfill via BigQuery public dataset (< 1 hour total)"
      - "11 ML-optimized columns (0.97 GB query cost, 0.1% of free tier)"
      - "Parquet intermediate format for zero-copy DuckDB import"
      - "Bitcoin 5-year backfill via mempool.space API (deferred to Phase 2+)"
    time_estimates:
      ethereum_bigquery: "< 1 hour (30min auth + 5min validation + 30-60min download)"
      ethereum_rpc_avoided: "26 days with Alchemy (624x slower, NOT using)"
      bitcoin: "Deferred to Phase 2+"
    reference: "Defines Phase 1 collection mode (BigQuery bulk download, NOT RPC streaming)"

  motherduck_integration:
    file: "/Users/terryli/eon/gapless-network-data/specifications/motherduck-integration.yaml"
    status: "operational"
    created_date: "2025-11-09"
    scope: "Production deployment of MotherDuck dual-pipeline Ethereum data collection"
    priority: "P0"
    environment: "production"
    deployment_status: "operational (deployed 2025-11-09)"
    components:
      - "Cloud Run Job: eth-md-updater (BigQuery → MotherDuck hourly sync)"
      - "Compute Engine: eth-realtime-collector (Alchemy WebSocket streaming)"
      - "MotherDuck: ethereum_mainnet.blocks (5,583+ blocks)"
      - "Secret Manager: motherduck-token, alchemy-api-key"
    slos:
      availability: "Data pipelines run without manual intervention"
      correctness: "100% data accuracy with no silent errors"
      observability: "100% operation tracking with queryable logs"
      maintainability: "<30 minutes for common operations"
    cost: "$0/month (all services within free tier)"
    migration_history:
      - "Phase 1: POC (Doppler validation, MotherDuck connection, BigQuery pipeline)"
      - "Phase 2: Production deployment (insecure Doppler credentials)"
      - "Phase 3: Secret Manager migration (best practice security)"
      - "Phase 4: Organization (hierarchical file structure, documentation)"
    next_steps:
      - "Git migration (6-9 hours, move production files to deployment/)"
      - "Historical backfill execution (2 hours, 13M blocks)"

x-implementation-findings:
  finding_1:
    date: "2025-11-03"
    phase: "phase_0_foundation"
    topic: "DuckDB optimal use case"
    discovery: |
      DuckDB should be used as query engine, not persistent storage.
      Parquet provides 110x storage savings for immutable time-series data.
    impact: "Architecture revised: Parquet for Data, DuckDB for Queries"
    action: "Update ValidationStorage design to use Parquet backend"
    status: "Integrated into Phase 1 feature implementations"

  finding_2:
    date: "2025-11-03"
    phase: "phase_0_foundation"
    topic: "ASOF JOIN prevents data leakage"
    discovery: |
      ASOF JOIN with ON ohlcv.timestamp >= mempool.timestamp ensures
      forward-fill semantics without lookahead bias. Critical for trading models.
    impact: "16x faster than pandas reindex, prevents future information leakage"
    action: "Prioritize ASOF JOIN for feature engineering integration"
    status: "Part of Phase 1 feature_engineering_integration"

  finding_3:
    date: "2025-11-03"
    phase: "phase_0_foundation"
    topic: "Gap detection and anomaly detection not implemented"
    discovery: |
      CLAUDE.md mentions "gap detection logic" and "z-score on vsize, fee spikes"
      but neither have implementations. DuckDB provides 10-20x faster solutions.
    impact: "Core features mentioned but not built"
    action: "Implement as part of 5-layer validation pipeline"
    status: "Part of Phase 1 bitcoin_mempool_collection"

  finding_4:
    date: "2025-11-03"
    phase: "phase_0_foundation"
    topic: "Production evidence validates approach"
    discovery: |
      DoorDash uses DuckDB for same use case: 1-minute intervals, z-score
      anomaly detection, <10 minutes vs hours with Spark.
    impact: "Reduces implementation risk, validates architecture"
    action: "Reference DoorDash case study in documentation"
    status: "Supporting evidence for implementation choices"

  finding_5:
    date: "2025-11-04"
    phase: "phase_1_planning"
    topic: "Roadmap refocus: Features vs Architecture"
    discovery: |
      Initial roadmap (duckdb-integration-strategy.yaml) focused heavily on
      implementation architecture (DuckDB optimization, Parquet migration, query
      performance) rather than user-facing features (Ethereum collection, multi-chain
      support, network insights).

      User feedback: "you seem to be leaning a lot on the architecture instead of
      what we want to do which startled me"
    impact: "Complete roadmap revision to feature-driven planning"
    action: |
      Revised master-project-roadmap.yaml to focus on:
      - Phase 1: Bitcoin + Ethereum collection (WHAT users get)
      - Phase 2: Multi-chain expansion + analytics (user capabilities)
      - Phase 3: Monitoring + optimization (production features)
      - DuckDB becomes HOW we implement features, not the features themselves
    status: "Roadmap revised (2025-11-04)"

x-cross-package-integration:
  strategy: "Separate databases, parallel patterns"
  rationale: |
    Investigation confirmed separate databases win 9-2 over unified approach.
    Benefits: package independence, schema evolution, isolated testing.

  databases:
    gapless_crypto_data:
      location: "~/.cache/gapless-crypto-data/validation.duckdb"
      status: "Fully implemented (v3.3.0)"
      schema: "OHLCV validation reports"
      reference: "/Users/terryli/eon/gapless-crypto-data/src/gapless_crypto_data/validation/storage.py"

    gapless_network_data:
      location: "~/.cache/gapless-network-data/validation.duckdb"
      status: "Planned (Phase 1)"
      schema: "Mempool validation reports"
      design: "Parquet backend (not DuckDB tables)"

  integration_options:
    pandas_join:
      approach: "Export both to pandas, join on timestamp"
      performance: "Good for <1M rows"
      use_case: "Default integration pattern"

    duckdb_asof_join:
      approach: "Query both Parquet datasets with ASOF JOIN"
      performance: "16x faster, prevents data leakage"
      use_case: "Feature engineering, large datasets"

    duckdb_attach:
      approach: "ATTACH both databases for SQL joins"
      performance: "Advanced SQL users"
      use_case: "Cross-validation analytics"

x-version-tracking:
  current_version: "v0.1.0"
  next_milestones:
    - version: "v0.2.0"
      scope: "Phase 1: Basic Data Collection"
      features:
        - "Ethereum block data collection via LlamaRPC (PRIMARY - 12s intervals)"
        - "Bitcoin mempool snapshots via mempool.space (SECONDARY - 5min intervals)"
        - "Multi-chain API: fetch_snapshots(chain='ethereum'|'bitcoin')"
        - "Basic validation (HTTP/RPC + schema)"
      estimated_release: "1-2 weeks"

    - version: "v0.3.0"
      scope: "Phase 2: Data Quality & Validation"
      features:
        - "Complete 5-layer validation pipeline"
        - "Gap detection and automatic backfill (Ethereum)"
        - "Zero-gap guarantee for Ethereum"
        - "ValidationStorage with Parquet backend"
      estimated_release: "1-2 weeks after v0.2.0"

    - version: "v0.4.0"
      scope: "Phase 3: Feature Engineering"
      features:
        - "Temporal alignment API (align_with_ohlcv)"
        - "Cross-domain features (10+ features documented)"
        - "Integration with gapless-crypto-data"
        - "43-feature engineering script"
      estimated_release: "1-2 weeks after v0.3.0"

    - version: "v0.5.0"
      scope: "Phase 4: Multi-Chain Expansion"
      features:
        - "Solana, Avalanche, Polygon support (5+ chains total)"
        - "Full 26-field Ethereum schema"
        - "Historical backfill for Ethereum (2015-2025)"
      estimated_release: "2-3 weeks after v0.4.0"

    - version: "v0.6.0"
      scope: "Phase 5: Production Features"
      features:
        - "Production CLI (stream, backfill, validate, export)"
        - "Real-time monitoring and alerting"
        - "Performance optimization (10x+ for large datasets)"
      estimated_release: "2-3 weeks after v0.5.0"

    - version: "v1.0.0"
      scope: "Phase 6: Release"
      requirements:
        - "All documentation complete (9 pending docs)"
        - "70%+ test coverage"
        - "CI/CD pipeline operational"
        - "PyPI published"
        - "Community resources available"
      estimated_release: "10-14 weeks total from Phase 1 start"

x-decision-log:
  - date: "2025-11-08"
    decision: "Use BigQuery for Ethereum historical backfill (supersedes all RPC approaches)"
    rationale: |
      BigQuery public dataset provides 624x faster historical data acquisition compared to
      RPC polling approaches (< 1 hour vs 26 days with Alchemy, 110 days with LlamaRPC).

      Key advantages:
      - Speed: < 1 hour total (30min auth + 5min validation + 30-60min download)
      - Cost: $0 (0.97 GB = 0.1% of 1 TB free tier per month)
      - Completeness: 100% data availability (no missing blocks, no rate limit failures)
      - Column optimization: 11 ML-optimized fields = 97% cost savings vs all 23 columns
      - Simplicity: No checkpoint/resume, rate limiting, or retry logic needed
      - Zero setup: No API keys, no RPC provider signup, just gcloud auth

      RPC streaming (real-time forward collection) deferred to Phase 2+ for incremental updates.
    impact: "Phase 1 timeline: 26 days → < 1 hour (624x speedup)"
    supersedes:
      - "2025-11-05 RPC provider comparison (Alchemy vs LlamaRPC decision)"
      - "All RPC-based collection approaches (LlamaRPC, Alchemy, Infura, QuickNode)"
    alternatives:
      - option: "Alchemy RPC (300M CU/month free tier)"
        rejected: "26 days runtime, rate limiting complexity, API key required"
        timeline: "26 days (5.79 RPS sustained)"
      - option: "LlamaRPC free tier"
        rejected: "110 days runtime (1.37 RPS sustainable), 4x slower than Alchemy"
        timeline: "110 days (empirically validated)"
      - option: "QuickNode free tier"
        rejected: "~157 days estimated, unverified rate limits"
      - option: "Infura free tier"
        rejected: "519 days (25K requests/day limit = 0.29 RPS), unusable"
    status: "Decided (2025-11-08), ready to implement"
    phase_impact:
      - "Phase 1: BigQuery bulk download (< 1 hour) instead of RPC polling (26-110 days)"
      - "Phase 2: Add RPC streaming for real-time forward collection (incremental updates)"
      - "Phase 3+: Feature engineering on complete 5-year historical dataset"
    skill_reference: "/Users/terryli/eon/gapless-network-data/.claude/skills/bigquery-ethereum-data-acquisition/"
    validation_status: "Empirically validated (v0.2.0, 2025-11-07)"

  - date: "2025-11-04"
    decision: "DuckDB PRIMARY for raw data storage (supersedes 2025-11-03 Parquet decision)"
    rationale: |
      User requirement is historical data collection + flexible feature engineering.
      DuckDB provides superior flexibility for:
      - Resampling (time_bucket for any interval)
      - Temporal joins (ASOF JOIN with OHLCV data)
      - Feature engineering (window functions, aggregations, z-scores)
      - Ad-hoc exploration (SQL console)

      Storage size is negligible (~1.5 GB for 5 years) - NOT "big data".
      110x savings only applies to validation reports, not time-series raw data.
    impact: "Architecture: Single DuckDB file for all raw data, export to Parquet for backup"
    storage_comparison:
      duckdb_table: "~1.5 GB for 13M Ethereum blocks (5 years, 76-100 bytes/block empirically validated)"
      parquet_files: "~1.2 GB for same data"
      difference: "300 MB (negligible, NOT 110x)"
    benefits:
      - "10-100x faster queries vs Pandas"
      - "Direct SQL feature engineering (no Parquet → Pandas → processing)"
      - "Simple architecture (one data.duckdb file vs 365+ Parquet files)"
      - "Maximum flexibility (time_bucket, ASOF JOIN, window functions)"
    alternatives:
      - option: "Parquet for Data, DuckDB for Queries (2025-11-03 decision)"
        rejected: |
          Complexity: 365+ Parquet files per year, file management overhead
          Slower: Must load Parquet → Pandas → process → write back
          Less flexible: Cannot resample directly, need Pandas operations
      - option: "Hybrid (Parquet immutable storage + DuckDB views)"
        rejected: "Unnecessary complexity for ~500 MB dataset"
    supersedes: "2025-11-03 'Parquet for all data storage' decision"
    status: "Decided (2025-11-04), refactoring all specifications"

  - date: "2025-11-03"
    decision: "Use Parquet for all data storage (not DuckDB tables)"
    status: "SUPERSEDED by 2025-11-04 DuckDB PRIMARY decision"
    rationale: "110x storage savings, immutable time-series optimal for Parquet"
    impact: "Architecture revised: Parquet for Data, DuckDB for Queries"
    note: "This decision was based on validation report storage analysis, not raw time-series data"
    alternatives:
      - option: "DuckDB tables"
        rejected: "537KB vs 4.9KB for same data"
      - option: "CSV files"
        rejected: "No columnar storage, poor query performance"

  - date: "2025-11-03"
    decision: "Prioritize ASOF JOIN as P0 feature"
    rationale: "Core feature engineering use case, 16x faster, prevents data leakage"
    impact: "Phase 1 task #2 (2-4 hours)"
    alternatives:
      - option: "pandas reindex"
        rejected: "Slower, lookahead bias risk"
      - option: "Manual merge_asof"
        rejected: "ASOF JOIN cleaner, more SQL-native"

  - date: "2025-11-03"
    decision: "Separate databases for gapless-crypto-data and gapless-network-data"
    rationale: "Package independence, isolated testing, schema evolution flexibility"
    impact: "Cross-package integration uses pandas join or DuckDB ATTACH"
    score: "9-2 in favor of separation"
    alternatives:
      - option: "Unified database"
        rejected: "Tight coupling, complex migrations"

  - date: "2025-11-04"
    decision: "Refocus roadmap on features (WHAT) vs architecture (HOW)"
    rationale: |
      Initial DuckDB-focused roadmap emphasized implementation architecture rather
      than user-facing capabilities. User feedback indicated this lost sight of
      actual feature goals (Ethereum collection, multi-chain support, network insights).
    impact: "Complete roadmap revision to feature-driven planning"
    changes:
      - "Phase 1: Core Features (Bitcoin + Ethereum collection)"
      - "Phase 2: Advanced Features (multi-chain, analytics, CLI)"
      - "Phase 3: Production Optimization (monitoring, performance)"
      - "DuckDB optimizations now implementation details, not features"
    alternatives:
      - option: "Keep DuckDB-focused roadmap"
        rejected: "Too architecture-heavy, loses sight of user value"
    status: "Implemented (master-project-roadmap.yaml revised)"

  - date: "2025-11-04"
    decision: "Prioritize Ethereum (LlamaRPC) as PRIMARY, Bitcoin as SECONDARY"
    rationale: |
      Research validation revealed:
      - Ethereum LlamaRPC: ~12 second block intervals (TRUE high-frequency)
      - Bitcoin mempool.space: M5 (5-minute) recent, H12 (12-hour) historical (LOW-frequency)

      Only Ethereum provides the high-frequency data needed for trading models and ML pipelines.
    impact: "Ethereum becomes Phase 1 priority (P0), Bitcoin becomes P1 (secondary)"
    changes:
      - "Phase 1: Ethereum first (6-8 hours), Bitcoin second (4-6 hours)"
      - "Success gates prioritize Ethereum collection"
      - "Bitcoin acknowledged as 5-min granularity limitation"
      - "Zero-gap guarantee only applicable to Ethereum"
    data_granularity:
      ethereum: "12 seconds (300 blocks/hour)"
      bitcoin: "5 minutes (12 snapshots/hour)"
    alternatives:
      - option: "Prioritize Bitcoin (mempool was in project name)"
        rejected: "Only 5-min granularity, not suitable for high-frequency trading"
    status: "Implemented (roadmap reorganized with Ethereum PRIMARY)"

  - date: "2025-11-04"
    decision: "Chronological phase ordering (6 phases)"
    rationale: |
      Logical dependency order ensures each phase builds on previous:
      1. Collection (prove we can get data)
      2. Quality (prove data is reliable)
      3. Features (prove data is useful)
      4. Expansion (scale horizontally - more chains)
      5. Production (scale vertically - monitoring, CLI, performance)
      6. Release (ship it)
    impact: "Clear progression from basic to production-ready"
    phases:
      - "Phase 1: Basic Collection (Ethereum + Bitcoin)"
      - "Phase 2: Data Quality & Validation (5-layer pipeline, gap detection)"
      - "Phase 3: Feature Engineering (temporal alignment, cross-domain)"
      - "Phase 4: Multi-Chain Expansion (Solana, Avalanche, Polygon)"
      - "Phase 5: Production Features (CLI, monitoring, alerts)"
      - "Phase 6: Release v1.0.0 (docs, tests, PyPI)"
    alternatives:
      - option: "Put feature engineering in Phase 1"
        rejected: "Can't engineer features without reliable data first"
      - option: "Put multi-chain expansion in Phase 2"
        rejected: "Should prove single-chain works before expanding"
    status: "Implemented (6-phase structure)"

  - date: "2025-11-04"
    decision: "Phase 1 Scope: Ethereum PRIMARY + Bitcoin SECONDARY (Option A)"
    rationale: |
      SSoT audit revealed two conflicting Phase 1 specifications:
      - Option A: master-project-roadmap.yaml (Ethereum + Bitcoin multi-chain)
      - Option B: core-collection-phase1.yaml (Bitcoin-only, 3/6 tasks complete)

      Chose Option A because:
      1. Aligns with package rename (gapless-mempool-data → gapless-network-data)
      2. Delivers high-frequency data (~12s Ethereum) needed for trading models
      3. web3.py already in dependencies (ready for Ethereum)
      4. Validates multi-chain architecture from Day 1
      5. Matches master-project-roadmap.yaml as written
    impact: "Phase 1 = Ethereum (P0) + Bitcoin (P1) in v0.2.0, duration 2 weeks"
    changes:
      - "Ethereum collector: 6-8 hours implementation (NEW)"
      - "Multi-chain API design: 2-3 hours (NEW)"
      - "Bitcoin collector: keep existing (3/6 tasks complete)"
      - "ETag caching: defer to Phase 2 (lower priority)"
      - "ValidationStorage: defer to Phase 2 (not blocking collection)"
    timeline:
      preparation: "6-10 hours (Ethereum spec, multi-chain API design)"
      implementation: "10-14 hours (Ethereum collector, Bitcoin completion, multi-chain API)"
      testing: "4-6 hours (70%+ coverage, integration tests)"
      total: "2 weeks (20-30 hours effort)"
    deliverables:
      - "Ethereum block collection (12s intervals, PRIMARY)"
      - "Bitcoin mempool snapshots (5min intervals, SECONDARY)"
      - "Multi-chain API: fetch_snapshots(chain='ethereum'|'bitcoin')"
      - "Basic validation (HTTP/RPC + schema) for both chains"
      - "70%+ test coverage"
      - "CLI with multi-chain support"
    alternatives:
      - option: "Option B: Bitcoin-only v0.2.0, defer Ethereum to v0.3.0"
        rejected: |
          While faster (1 week vs 2 weeks) and lower risk (single-chain), this choice:
          - Delays high-frequency data delivery (only 5-min Bitcoin granularity)
          - Leaves package name mismatch (gapless-network-data implies multi-chain)
          - Postpones multi-chain validation
          - Doesn't align with master roadmap vision
    superseded_specifications:
      - file: "core-collection-phase1.yaml"
        status: "superseded"
        reason: "Bitcoin-only scope conflicts with multi-chain Phase 1"
        action: "Archived to specifications/archive/ (2025-11-04)"
        reusable_content:
          - "Retry logic patterns (completed)"
          - "Structured exceptions (completed)"
          - "Forward-collection enforcement (completed)"
    status: "Decided (2025-11-04), implementing"

x-risk-assessment:
  implementation_risks:
    - risk: "ValidationStorage migration complexity"
      severity: "low"
      mitigation: "Copy from gapless-crypto-data, proven implementation"

    - risk: "DuckDB learning curve"
      severity: "low"
      mitigation: "Comprehensive investigation reports, 23 features documented"

    - risk: "Performance expectations not met"
      severity: "medium"
      mitigation: "Benchmarks with acceptance criteria (10x minimum speedup)"

  schedule_risks:
    - risk: "Phase 1 takes longer than 14-19 hours"
      severity: "low"
      mitigation: "Tasks estimated conservatively, can defer low-priority features"

    - risk: "Testing reveals correctness issues"
      severity: "medium"
      mitigation: "Unit tests verify against reference implementations (pandas, scipy)"

  technical_debt:
    - debt: "Gap detection and anomaly detection mentioned but not implemented"
      severity: "high"
      resolution: "Phase 1 implements both (6-8 hours combined)"

    - debt: "Documentation incomplete (9 files pending)"
      severity: "medium"
      resolution: "Phase 4 completes all documentation"

x-success-metrics:
  code_quality:
    - metric: "Test coverage"
      target: "70%+ for SDK, validation, collectors"
      current: "~30% (basic tests only)"

    - metric: "Type coverage"
      target: "100% (mypy strict mode)"
      current: "100% (py.typed marker present)"

  performance:
    - metric: "Storage efficiency"
      target: "110x savings (Parquet vs DuckDB tables)"
      baseline: "537KB (DuckDB) → 4.9KB (Parquet)"

    - metric: "Analytics speedup"
      target: "10-100x (DuckDB vs pandas)"
      benchmarks:
        - "ASOF JOIN: 16x (800ms → 50ms)"
        - "Gap detection: 20x (1200ms → 60ms)"
        - "Z-score: 10x (300ms → 30ms)"

  documentation:
    - metric: "Documentation completeness"
      target: "100% (all pending docs completed)"
      current: "~40% (4/9 guide docs pending)"

    - metric: "Code example coverage"
      target: "All features have working examples"
      current: "6/6 DuckDB features have examples in spec"

x-references:
  investigation_reports:
    - path: "/tmp/duckdb-capabilities/EXECUTIVE_SUMMARY.md"
      content: "23 features, priority matrix, 3-week roadmap"

    - path: "/tmp/duckdb-current-usage/duckdb-usage-audit-report.md"
      content: "0% implementation gap, referential implementation guidance"

    - path: "/tmp/duckdb-parquet/REPORT.md"
      content: "110x storage savings, zero-copy queries, benchmarks"

    - path: "/tmp/duckdb-analytics/analytics_query_patterns_report.md"
      content: "10-100x speedups, 8 query templates, API design"

    - path: "/tmp/duckdb-cross-package/EXECUTIVE_SUMMARY.md"
      content: "Separate databases strategy, integration options"

  project_documentation:
    - path: "/Users/terryli/eon/gapless-network-data/CLAUDE.md"
      content: "Project memory, navigation hub, DuckDB strategy"

    - path: "/Users/terryli/eon/gapless-network-data/specifications/duckdb-integration-strategy.yaml"
      content: "23 features, implementation roadmap, acceptance criteria"

    - path: "/Users/terryli/eon/gapless-network-data/specifications/documentation-audit-phase.yaml"
      content: "Documentation audit findings and resolutions (completed)"
