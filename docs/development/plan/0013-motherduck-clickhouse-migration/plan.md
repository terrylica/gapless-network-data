# Plan 0013: MotherDuck to ClickHouse AWS Migration

**ADR ID**: 0013
**Status**: In Progress
**Created**: 2025-11-24
**Related MADR**: `docs/decisions/0013-motherduck-clickhouse-migration.md`

## (a) Context

**Why This Plan Exists**:

MotherDuck trial ending in 2-3 days requires urgent migration to ClickHouse Cloud AWS. Current infrastructure:

**Production Data**:

- 23.84M Ethereum blocks (verified 2025-11-20 via MotherDuck)
- ~1.5 GB storage (76-100 bytes/block)
- Dual-pipeline: BigQuery hourly (578 blocks/run) + Alchemy real-time (12s intervals)

**Infrastructure to Migrate**:
| Component | File | MotherDuck Usage |
|-----------|------|------------------|
| VM Real-Time Collector | `deployment/vm/realtime_collector.py` | Line 121: `duckdb.connect('md:')` |
| Cloud Run Batch Updater | `deployment/cloud-run/main.py` | Line 136: `duckdb.connect('md:')` |
| Data Quality Checker | `deployment/cloud-run/data_quality_checker.py` | Lines 70-73 |
| Gap Monitor Function | `deployment/gcp-functions/motherduck-monitor/main.py` | Line 361 |

**ClickHouse Target**:

- Service: ClickHouse Cloud (managed AWS)
- Status: Provisioned but untested
- Credentials: Doppler `aws-credentials/prd` (8 secrets)

**Constraints**:

- Zero downtime required
- 2-3 days to trial expiration
- Fail-fast error policy (exception-only)
- 6-12 hour compressed validation

## (b) Plan

**Objective**: Migrate 23.84M Ethereum blocks and 4 production pipelines from MotherDuck to ClickHouse AWS with zero downtime.

**Strategy**: Dual-write (both databases active during migration, safe rollback at every stage)

**Timeline** (2.5 days compressed):

### Day 0 (TODAY) - Pre-Migration & Historical Load

| Hour | Task                           | Success Criteria                             |
| ---- | ------------------------------ | -------------------------------------------- |
| 0-1  | Validate ClickHouse connection | Query returns result, write succeeds         |
| 1-2  | Create ClickHouse schema       | Table created with ReplacingMergeTree        |
| 2-6  | Historical data migration      | 23.84M rows in ClickHouse, row count matches |

### Day 1 - Dual-Write Deployment

| Hour | Task                           | Success Criteria                       |
| ---- | ------------------------------ | -------------------------------------- |
| 0-2  | Update 4 production components | Code changes complete, tests pass      |
| 2-4  | Deploy to GCP                  | VM, Cloud Run, Cloud Function deployed |
| 4-16 | Compressed validation          | Hourly consistency checks pass         |

### Day 2 - Cutover & Deprecation

| Hour | Task                   | Success Criteria                        |
| ---- | ---------------------- | --------------------------------------- |
| 0-2  | Read cutover           | SDK/monitoring reads from ClickHouse    |
| 2-4  | MotherDuck deprecation | Writes stopped, final snapshot archived |

**Rollback Strategy**:

- Before Day 1 Hour 4: No changes to production (revert branch)
- Day 1-2: Revert to MotherDuck-only (dual-write continues, switch reads back)
- After Day 2: MotherDuck archived in GCS (30-day retention for emergency recovery)

## (c) Task List

### Phase 0: Validation (Day 0, Hour 0-1) ‚úÖ COMPLETED

**0.1 Validate ClickHouse connection using Doppler credentials** ‚úÖ COMPLETED

- Command: `doppler run --project aws-credentials --config prd -- uv run scripts/clickhouse/validate_connection.py`
- Result: Connection succeeded, server version 25.8.1.8702
- Validated: 2025-11-24T23:20:10
- Output: `/tmp/clickhouse-validation.txt`

### Phase 1: Schema & Historical Migration (Day 0, Hour 1-6) ‚úÖ COMPLETED

**1.1 Create ClickHouse schema** ‚úÖ COMPLETED

- Engine: ReplacingMergeTree (automatic deduplication on background merge)
- ORDER BY: `number` (block number as deduplication key)
- PARTITION BY: `toYYYYMM(timestamp)` (monthly partitions)
- Script: `scripts/clickhouse/create_schema.py`
- Executed: 2025-11-24T23:21:04

**1.2 Export MotherDuck to Parquet** ‚è≠Ô∏è SKIPPED

- Decision: Migrate directly from BigQuery (source of truth) instead of MotherDuck
- Rationale: BigQuery has all historical data, no MotherDuck SSO timeout issues

**1.3 BigQuery ‚Üí ClickHouse Migration** ‚úÖ COMPLETED

- Script: `scripts/clickhouse/migrate_from_bigquery.py`
- Total rows: **23,865,016 blocks** (2015-2025)
- Duration: 5 minutes (300 seconds)
- Rate: ~80K rows/sec
- Verification: Row counts per year verified
- Completed: 2025-11-24T23:35:00

### Phase 2: Dual-Write Implementation (Day 1, Hour 0-4) - CODE COMPLETE ‚úÖ

**Phase 2 Summary** (completed 2025-11-25):

- All 4 production components updated with dual-write support
- Code validated (Python syntax checks pass for all files)
- GCP secrets created via Python SDK (`setup_gcp_secrets.py`)
- Verification scripts created (`verify_consistency.py`, `setup_gcp_secrets.sh`)
- Deployment guide created (`DEPLOYMENT.md`)
- **Next**: Deploy to GCP via Cloud Console (gcloud CLI not available locally)

**2.1 Update VM realtime collector** ‚úÖ COMPLETED

- File: `deployment/vm/realtime_collector.py`
- Changes: Added ClickHouse client, dual-write in `flush_buffer()` and `insert_block()`
- Error policy: Fail-fast (ClickHouse written FIRST, exception propagated on failure)
- Features: `DUAL_WRITE_ENABLED` env var toggle, `flush_to_clickhouse()` helper
- Completed: 2025-11-24

**2.2 Update Cloud Run batch updater** ‚úÖ COMPLETED

- File: `deployment/cloud-run/main.py`
- Changes: Added `load_to_clickhouse()` function, dual-write in main flow
- Error policy: Fail-fast (ClickHouse written FIRST, exception propagated on failure)
- Features: `DUAL_WRITE_ENABLED` env var toggle, pandas df conversion with UInt256 handling
- Completed: 2025-11-24

**2.3 Update Cloud Run data quality checker** ‚úÖ COMPLETED

- File: `deployment/cloud-run/data_quality_checker.py`
- Changes: Added `check_clickhouse_freshness()` function, CHECK_CLICKHOUSE env var toggle
- Features: Validates both MotherDuck and ClickHouse when enabled
- Dependencies: Added `clickhouse-connect` to inline script dependencies
- Completed: 2025-11-25

**2.4 Update Cloud Function gap monitor** ‚úÖ COMPLETED

- File: `deployment/gcp-functions/motherduck-monitor/main.py`
- Changes: Added `validate_clickhouse_sync()` function, cross-database comparison
- Features: `DUAL_VALIDATION_ENABLED` env var, block count/max comparison with tolerance
- Dependencies: Added `clickhouse-connect>=0.7.0` to requirements.txt
- Completed: 2025-11-24

**2.5 Deploy to GCP** ‚úÖ COMPLETED

> **Detailed Guide**: See [`DEPLOYMENT.md`](./DEPLOYMENT.md) for step-by-step deployment instructions.

**Deployment Summary** (2025-11-25):

- ‚úÖ GCP secrets created via Python SDK (`setup_gcp_secrets.py`)
- ‚úÖ VM collector deployed with dual-write (confirmed in logs)
- ‚úÖ Cloud Run job `eth-md-updater` updated with dual-write env vars
- ‚úÖ Cloud Function `motherduck-gap-detector` updated with dual-validation

**Dual-Write Verification**:

```
[BATCH] ‚úÖ Flushed 25 blocks to ClickHouse
[BATCH] ‚úÖ Flushed 25 blocks to MotherDuck
```

**Consistency Check** (2025-11-25T08:03):

- ClickHouse: 23,865,042 blocks (max: 23,874,501)
- MotherDuck: 23,874,495 blocks (max: 23,874,501)
- Max block matches: ‚úÖ Both databases in sync for new blocks
- Historical diff: ~9,453 blocks (expected from BigQuery migration timing)

**Quick Deployment:**

```bash
# Step 1: Setup GCP secrets (requires gcloud CLI)
./scripts/clickhouse/setup_gcp_secrets.sh

# Step 2: Deploy VM collector
cd deployment/vm && ./deploy.sh

# Step 3: Deploy Cloud Run job
gcloud run jobs update bigquery-motherduck-updater \
  --set-env-vars="CLICKHOUSE_HOST=$CH_HOST,CLICKHOUSE_PASSWORD=$CH_PASS,DUAL_WRITE_ENABLED=true" \
  --region=us-east1 --project=eonlabs-ethereum-bq

# Step 4: Deploy Cloud Function
cd deployment/gcp-functions/motherduck-monitor
gcloud functions deploy motherduck-monitor \
  --gen2 --runtime=python312 --trigger-http \
  --set-env-vars="CLICKHOUSE_HOST=$CH_HOST,CLICKHOUSE_PASSWORD=$CH_PASS,DUAL_VALIDATION_ENABLED=true" \
  --region=us-east1 --project=eonlabs-ethereum-bq
```

### Phase 3: Compressed Validation (Day 1, Hour 4-16) - IN PROGRESS

**3.1 Run hourly consistency checks** üîÑ STARTED (2025-11-25T08:02)

- Script: `scripts/clickhouse/verify_consistency.py`
- Checks: Row count, max block number
- Key metric: Max block must match (indicates dual-write sync)
- Alert: Pushover notification on discrepancy
- Initial check: Max block 23,874,501 in both databases ‚úÖ

### Phase 4: Cutover (Day 2, Hour 0-4)

**4.1 Switch reads to ClickHouse** ‚è≥ PENDING

- Update environment variable: `PRIMARY_DATABASE=clickhouse`
- Verify monitoring reads from ClickHouse

**4.2 Stop MotherDuck writes** ‚è≥ PENDING

- Remove dual-write code (ClickHouse-only)
- Redeploy all 4 components

**4.3 Archive MotherDuck snapshot** ‚è≥ PENDING

- Export final state to GCS
- Delete MotherDuck database
- Retain backup 30 days

### Phase 5: Documentation Updates

**5.1 Update CLAUDE.md** ‚è≥ PENDING

- Replace MotherDuck references with ClickHouse
- Update architecture diagrams
- Update Data Storage Architecture section

**5.2 Update deployment READMEs** ‚è≥ PENDING

- `deployment/vm/README.md`
- `deployment/cloud-run/README.md`
- `deployment/gcp-functions/motherduck-monitor/README.md`

**5.3 Update master-project-roadmap.yaml** ‚è≥ PENDING

- Update infrastructure section
- Add ClickHouse as production database

## Success Criteria

- ‚úÖ ClickHouse connection validated (2025-11-24T23:20:10)
- ‚úÖ Schema created with ReplacingMergeTree (2025-11-24T23:21:04)
- ‚úÖ 23.86M blocks migrated (BigQuery ‚Üí ClickHouse in 5 minutes)
- ‚úÖ Dual-write code complete for all 4 components (VM, Cloud Run job, Cloud Run checker, Cloud Function)
- ‚úÖ GCP secrets created via Python SDK (2025-11-25)
- ‚úÖ Dual-write deployed to production (2025-11-25T08:02)
- ‚úÖ Dual-write verified in VM logs (both databases receiving new blocks)
- ‚è≥ 6-12 hour validation period (STARTED 2025-11-25T08:02)
- ‚è≥ Zero data loss during cutover
- ‚è≥ All documentation updated

## Verification Scripts

**Location**: `scripts/clickhouse/`

| Script                     | Purpose                                            | Status      |
| -------------------------- | -------------------------------------------------- | ----------- |
| `validate_connection.py`   | Test ClickHouse connectivity                       | ‚úÖ Created  |
| `create_schema.py`         | Create ReplacingMergeTree table                    | ‚úÖ Created  |
| `migrate_from_bigquery.py` | Direct BigQuery ‚Üí ClickHouse migration             | ‚úÖ Created  |
| `verify_consistency.py`    | Hourly ClickHouse ‚Üî MotherDuck comparison         | ‚úÖ Created  |
| `setup_gcp_secrets.sh`     | Create secrets via gcloud CLI                      | ‚úÖ Created  |
| `setup_gcp_secrets.py`     | Create secrets via Python SDK (no gcloud required) | ‚úÖ Executed |

**Verification Command** (run hourly during validation):

```bash
doppler run --project aws-credentials --config prd -- uv run scripts/clickhouse/verify_consistency.py
```
